{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model tuning and hyperparameter optimization\n",
    "\n",
    "<i>Balázs Kégl (CNRS / PS-CDS) and Alex Gramfort (Inria)</i>\n",
    "\n",
    "## Introduction\n",
    "Virtually all learning models come with a set of parameters that should be tuned. To distinguish them from the parameters of the models set during training (learned parameters like neural weigths, tree cuts, or ensemble coefficients), we call them <i>hyperparameters</i>. Typical examples are the number of iterations and the tree depth in random forests, or the <code>C</code> or <code>gamma</code> of a support vector classifier. Scikit-learn has default values for all these parameters, but there is no guarantee that any of the classifiers perform optimally (or even close to optimally) using these default parameters. Worse: the optimal parameters vary depending on the data set, so tuning them is the task of the analyst, not the task of the provider of the library.\n",
    "\n",
    "Hyperparameters come in all shapes and types. Some of them are crucial to be tuned (like <code>n_estimators</code> in ensemble methods), the default value of some others works fine most of the time (say, <code>min_samples_split</code> of a random forest). Scikit-learn also has auxiliary parameters that have no effects on the model (such as <code>verbose</code>), so knowing which parameters to tune requires understanding a little bit what the particular model is about. Most of the hyperparameters have typical ranges, although they can depend of course on the characteristics of the data set. There are even so called <i>conditional</i> hyperparameters whose <i>existence</i> depend on the value of some other hyperparameters: for example, when you add a new layer to a neural network, suddenly you have 3-5 new hyperparameters to tune.\n",
    "\n",
    "Important hyperparameters can usually be interpreted as complexity parameters of the learned function. Sometimes the link is easy to see: an ensemble with a large number of deep trees have more degrees of freedom than a small ensemble of small trees. Sometimes the link between hyperparameters and complexity is more indirect, and understanding it requires some basic notion of <i>regularization</i>. One general notion that helps to understand the behavior of the learning algorithms with respect to their hyperparameters is <i>overfitting</i> and <i>underfitting</i> (this is why it is important to understand whether complexity increases or decreases as the hyperparameter grows). Let us call the model that we could obtain if we knew the distribution (or had infinite data) the <i>ideal</i> model. The goal of learning is to get as close to this model as possible, using only a finite training set. We may have two problems\n",
    "<ol>\n",
    "<li>\n",
    "Underfitting occurs when our model is not flexible enough to pick up the details of the ideal function, so even with infinite data we could not get close to the ideal function. The difference between the error of the ideal function and the error of the best function we could pick from our model class (parametrized by the hyperparameters) is the <i>approximation</i> error. Underfitting means that the approximation error is larger than optimal. Increasing the complexity of the model class decreases the approximation error (independently of the data set).\n",
    "<li>\n",
    "Overfitting occurs when our model is too flexible so it learns the training data \"by heart\" and does not generalize well to unseen test data. The difference between the error of the function (learned on the data set) and the error of the best function we could pick from our model class (parametrized by the hyperparameters) is the <i>estimation</i> error. Overfitting means that estimation error is larger than optimal. Increasing the complexity of the model class (while keeping the data set fixed) increases the estimation error.\n",
    "</ol>\n",
    "Note the crucial difference between the two notions: the level of underfitting is independent of the data set; it only depends on the data distribution and the function set (parametrized by the hyperparameters), whereas the level of overfitting, of course, depends on the data set. Here are some general rules that could guide you in this landscape.\n",
    "<ol>\n",
    "<li> Overfitting increases as the model complexity grows.\n",
    "<li> Overfitting increases as the data complexity (for example, the number of features) grows.\n",
    "<li> Overfitting decreases as the data size grows.\n",
    "</ol>\n",
    "\n",
    "If we could compute the data distribution or had access to an infinite data set we could determine the estimation and approximation errors and set the model complexity to minimize their sum. Of course, this is impossible so these notions are not constructive (but can help you to understand the underlying phenomena). In practice, we <i>estimate</i> the <i>generalization</i> error (the sum of the approximation and estimation errors) on a held out (validation) set. The pragmatics of hyperparameter optimization are thus relatively simple: find the hyperparameters that minimize the validation error. The practical difficulties are the following:\n",
    "<ol>\n",
    "<li> Our estimate has a variance: it is based on a finite validation set. Optimizing a \"noisy\" function can be tricky. Using cross validation schemes makes this even worse: since training and test sets overlap, complex correlation structures are introduced into the error estimates.\n",
    "<li> Evaluating the validation error at a vector of hyperparameter values requires training a model which may be slow. \n",
    "<li> The number of hyperparameter combinations increases exponentially with the number of hyperparameters.\n",
    "</ol> \n",
    "\n",
    "The first step of hyperparameter tuning is usually trial and error. Try some combinations and look at how the error behaves. Once you have a \"feeling\" about what range should be explored and the sensitivity of the error to the hyperparameters, you can do a grid search (using either <code>GridSearchCV</code> from scikit learn, or by coding it). In this notebook we coded a simple grid search so you understand how it works. Once the number of hyperparameters is large (say, larger than 2) grid search will become increasingly time consuming. The main inefficency is a result of it not being intelligent: it will exhaustively look at <i>all</i> combinations, even those that are \"trivially\" bad. \n",
    "\n",
    "The main principle to improve on grid search is smoothness in the hyperparameter space: we assume that models with similar hyperparameters are not very different. This allows sophisticated automatic methods to direct the search toward promising regions while also keeping exploring unexplored regions. Most of these methods are based on <a href=\"http://en.wikipedia.org/wiki/Bayesian_optimization\">Bayesian optimization</a>. We will work with the <a href=\"https://github.com/hyperopt/hyperopt/wiki/FMin\">hyperopt</a> package. It can be installed with:\n",
    "\n",
    "<code>pip install hyperopt</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the data and load it in pandas, and convert it into numpy\n",
    "We can use any data set here. By the end of this section, two numpy arrays, <code>X</code> and <code>y</code> shoud be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patch_id</th>\n",
       "      <th>star_id_b</th>\n",
       "      <th>star_id_r</th>\n",
       "      <th>magnitude_b</th>\n",
       "      <th>magnitude_r</th>\n",
       "      <th>asc_d</th>\n",
       "      <th>asc_m</th>\n",
       "      <th>asc_s</th>\n",
       "      <th>dec_d</th>\n",
       "      <th>dec_m</th>\n",
       "      <th>dec_s</th>\n",
       "      <th>period</th>\n",
       "      <th>frequency</th>\n",
       "      <th>num_points_good_b</th>\n",
       "      <th>num_points_good_r</th>\n",
       "      <th>asym_b</th>\n",
       "      <th>asym_r</th>\n",
       "      <th>log_p_not_variable</th>\n",
       "      <th>sigma_flux_b</th>\n",
       "      <th>sigma_flux_r</th>\n",
       "      <th>type</th>\n",
       "      <th>quality</th>\n",
       "      <th>div_period</th>\n",
       "      <th>real_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54_3198</th>\n",
       "      <td>54.0</td>\n",
       "      <td>3198.0</td>\n",
       "      <td>3327.0</td>\n",
       "      <td>19.0780</td>\n",
       "      <td>18.1298</td>\n",
       "      <td>5.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16.84</td>\n",
       "      <td>0.610837</td>\n",
       "      <td>1.637100</td>\n",
       "      <td>123.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>2.61765</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>-2.76059</td>\n",
       "      <td>41.4272</td>\n",
       "      <td>88.0977</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.610837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578_2223</th>\n",
       "      <td>578.0</td>\n",
       "      <td>2223.0</td>\n",
       "      <td>2598.0</td>\n",
       "      <td>19.0282</td>\n",
       "      <td>18.8067</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>52.85</td>\n",
       "      <td>-69.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>40.12</td>\n",
       "      <td>1.913020</td>\n",
       "      <td>0.522734</td>\n",
       "      <td>119.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.12500</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-2.23733</td>\n",
       "      <td>39.5553</td>\n",
       "      <td>45.3581</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.318837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242_3046</th>\n",
       "      <td>242.0</td>\n",
       "      <td>3046.0</td>\n",
       "      <td>3274.0</td>\n",
       "      <td>17.8534</td>\n",
       "      <td>15.6034</td>\n",
       "      <td>5.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>26.46</td>\n",
       "      <td>-69.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>32.21</td>\n",
       "      <td>0.996323</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>102.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.37209</td>\n",
       "      <td>1.365850</td>\n",
       "      <td>-3.58045</td>\n",
       "      <td>118.8270</td>\n",
       "      <td>939.3540</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          patch_id  star_id_b  star_id_r  magnitude_b  magnitude_r  asc_d  \\\n",
       "54_3198       54.0     3198.0     3327.0      19.0780      18.1298    5.0   \n",
       "578_2223     578.0     2223.0     2598.0      19.0282      18.8067    5.0   \n",
       "242_3046     242.0     3046.0     3274.0      17.8534      15.6034    5.0   \n",
       "\n",
       "          asc_m  asc_s  dec_d  dec_m  dec_s    period  frequency  \\\n",
       "54_3198    48.0  19.58  -70.0   36.0  16.84  0.610837   1.637100   \n",
       "578_2223    5.0  52.85  -69.0    8.0  40.12  1.913020   0.522734   \n",
       "242_3046   31.0  26.46  -69.0   14.0  32.21  0.996323   1.003690   \n",
       "\n",
       "          num_points_good_b  num_points_good_r   asym_b    asym_r  \\\n",
       "54_3198               123.0              122.0  2.61765  0.794118   \n",
       "578_2223              119.0              120.0  1.12500  0.600000   \n",
       "242_3046              102.0               97.0  1.37209  1.365850   \n",
       "\n",
       "          log_p_not_variable  sigma_flux_b  sigma_flux_r  type  quality  \\\n",
       "54_3198             -2.76059       41.4272       88.0977   3.0      2.0   \n",
       "578_2223            -2.23733       39.5553       45.3581   3.0      2.0   \n",
       "242_3046            -3.58045      118.8270      939.3540   3.0      1.0   \n",
       "\n",
       "          div_period  real_period  \n",
       "54_3198          1.0     0.610837  \n",
       "578_2223         6.0     0.318837  \n",
       "242_3046         1.0     0.996323  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\", index_col=0)\n",
    "data['real_period'] = data['period'] / data['div_period']\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[['magnitude_b', 'magnitude_r', 'real_period']].values\n",
    "y = data['type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3641, 3), (3641,), dtype('float64'), dtype('float64'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape, X.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're good !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier\n",
    "\n",
    "without worrying too much about hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9154 +/-0.0136\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=8)\n",
    "cv = KFold(n_splits=5, random_state=42)\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(\"Accuracy: {:.4f} +/-{:.4f}\".format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a classifier using a grid of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "5\n",
      "10\n",
      "20\n",
      "30\n",
      "50\n",
      "100\n",
      "200\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "n_estimators_list = [2, 3, 5, 10, 20, 30, 50, 100, 200, 500]\n",
    "max_depth_list = [1, 2, 3, 5, 7, 10, 15, 20, 30]\n",
    "accuracies = {}\n",
    "accuracy_stds = {}\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "    print(n_estimators)\n",
    "    for max_depth in max_depth_list:\n",
    "        clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, n_jobs=1)\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=3)\n",
    "        accuracies[(n_estimators, max_depth)] = np.mean(scores)\n",
    "        accuracy_stds[(n_estimators, max_depth)] = np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2, 1): 0.74120879120879124,\n",
       " (2, 2): 0.81923076923076921,\n",
       " (2, 3): 0.87417582417582407,\n",
       " (2, 5): 0.87857142857142845,\n",
       " (2, 7): 0.87032967032967046,\n",
       " (2, 10): 0.84230769230769231,\n",
       " (2, 15): 0.84175824175824177,\n",
       " (2, 20): 0.84285714285714286,\n",
       " (2, 30): 0.84285714285714286,\n",
       " (3, 1): 0.77527472527472518,\n",
       " (3, 2): 0.85109890109890107,\n",
       " (3, 3): 0.86373626373626367,\n",
       " (3, 5): 0.89890109890109893,\n",
       " (3, 7): 0.88736263736263721,\n",
       " (3, 10): 0.89230769230769236,\n",
       " (3, 15): 0.88681318681318688,\n",
       " (3, 20): 0.89230769230769236,\n",
       " (3, 30): 0.89230769230769236,\n",
       " (5, 1): 0.76208791208791204,\n",
       " (5, 2): 0.85109890109890107,\n",
       " (5, 3): 0.88406593406593414,\n",
       " (5, 5): 0.90274725274725276,\n",
       " (5, 7): 0.90714285714285714,\n",
       " (5, 10): 0.90879120879120878,\n",
       " (5, 15): 0.9054945054945055,\n",
       " (5, 20): 0.90879120879120878,\n",
       " (5, 30): 0.90879120879120878,\n",
       " (10, 1): 0.74450549450549453,\n",
       " (10, 2): 0.85329670329670326,\n",
       " (10, 3): 0.89340659340659345,\n",
       " (10, 5): 0.9054945054945055,\n",
       " (10, 7): 0.90384615384615385,\n",
       " (10, 10): 0.90989010989010988,\n",
       " (10, 15): 0.90494505494505495,\n",
       " (10, 20): 0.90494505494505495,\n",
       " (10, 30): 0.90494505494505495,\n",
       " (20, 1): 0.77142857142857135,\n",
       " (20, 2): 0.84780219780219779,\n",
       " (20, 3): 0.88901098901098907,\n",
       " (20, 5): 0.90714285714285714,\n",
       " (20, 7): 0.91373626373626371,\n",
       " (20, 10): 0.9148351648351648,\n",
       " (20, 15): 0.91208791208791207,\n",
       " (20, 20): 0.91098901098901097,\n",
       " (20, 30): 0.91098901098901097,\n",
       " (30, 1): 0.76593406593406588,\n",
       " (30, 2): 0.85109890109890107,\n",
       " (30, 3): 0.89120879120879126,\n",
       " (30, 5): 0.9054945054945055,\n",
       " (30, 7): 0.91318681318681316,\n",
       " (30, 10): 0.91208791208791207,\n",
       " (30, 15): 0.91373626373626371,\n",
       " (30, 20): 0.91153846153846152,\n",
       " (30, 30): 0.91153846153846152,\n",
       " (50, 1): 0.76758241758241752,\n",
       " (50, 2): 0.84120879120879111,\n",
       " (50, 3): 0.8824175824175825,\n",
       " (50, 5): 0.9054945054945055,\n",
       " (50, 7): 0.91318681318681316,\n",
       " (50, 10): 0.91428571428571426,\n",
       " (50, 15): 0.91373626373626371,\n",
       " (50, 20): 0.91648351648351645,\n",
       " (50, 30): 0.91648351648351645,\n",
       " (100, 1): 0.76373626373626369,\n",
       " (100, 2): 0.83846153846153848,\n",
       " (100, 3): 0.88901098901098907,\n",
       " (100, 5): 0.90824175824175823,\n",
       " (100, 7): 0.91428571428571426,\n",
       " (100, 10): 0.91648351648351645,\n",
       " (100, 15): 0.91703296703296699,\n",
       " (100, 20): 0.91758241758241754,\n",
       " (100, 30): 0.91758241758241754,\n",
       " (200, 1): 0.76483516483516478,\n",
       " (200, 2): 0.84065934065934067,\n",
       " (200, 3): 0.893956043956044,\n",
       " (200, 5): 0.91098901098901097,\n",
       " (200, 7): 0.91428571428571426,\n",
       " (200, 10): 0.91978021978021973,\n",
       " (200, 15): 0.91813186813186809,\n",
       " (200, 20): 0.91813186813186809,\n",
       " (200, 30): 0.91813186813186809,\n",
       " (500, 1): 0.76593406593406588,\n",
       " (500, 2): 0.83956043956043958,\n",
       " (500, 3): 0.8928571428571429,\n",
       " (500, 5): 0.91043956043956042,\n",
       " (500, 7): 0.91648351648351645,\n",
       " (500, 10): 0.91868131868131864,\n",
       " (500, 15): 0.91758241758241754,\n",
       " (500, 20): 0.91648351648351645,\n",
       " (500, 30): 0.91703296703296699}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91978021978021973"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(list(accuracies.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do it using scikit-learn <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">GridSearchCV</a> object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=42, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=3,\n",
       "       param_grid={'n_estimators': [2, 3, 5, 10, 20, 30, 50, 100, 200, 500], 'max_depth': [1, 2, 3, 5, 7, 10, 15, 20, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tuned_parameters = {'n_estimators': n_estimators_list, 'max_depth': max_depth_list}\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "gs = GridSearchCV(clf, tuned_parameters, cv=cv, scoring='accuracy', n_jobs=3)\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'n_estimators': 100, 'max_depth': 20}\n",
      "()\n",
      "Grid scores on training set:\n",
      "()\n",
      "0.776 (+/-0.018) for {'n_estimators': 2, 'max_depth': 1}\n",
      "0.768 (+/-0.020) for {'n_estimators': 3, 'max_depth': 1}\n",
      "0.810 (+/-0.018) for {'n_estimators': 5, 'max_depth': 1}\n",
      "0.774 (+/-0.019) for {'n_estimators': 10, 'max_depth': 1}\n",
      "0.775 (+/-0.022) for {'n_estimators': 20, 'max_depth': 1}\n",
      "0.769 (+/-0.012) for {'n_estimators': 30, 'max_depth': 1}\n",
      "0.770 (+/-0.015) for {'n_estimators': 50, 'max_depth': 1}\n",
      "0.770 (+/-0.012) for {'n_estimators': 100, 'max_depth': 1}\n",
      "0.771 (+/-0.011) for {'n_estimators': 200, 'max_depth': 1}\n",
      "0.771 (+/-0.011) for {'n_estimators': 500, 'max_depth': 1}\n",
      "0.885 (+/-0.020) for {'n_estimators': 2, 'max_depth': 2}\n",
      "0.880 (+/-0.020) for {'n_estimators': 3, 'max_depth': 2}\n",
      "0.873 (+/-0.015) for {'n_estimators': 5, 'max_depth': 2}\n",
      "0.829 (+/-0.046) for {'n_estimators': 10, 'max_depth': 2}\n",
      "0.807 (+/-0.035) for {'n_estimators': 20, 'max_depth': 2}\n",
      "0.806 (+/-0.046) for {'n_estimators': 30, 'max_depth': 2}\n",
      "0.827 (+/-0.042) for {'n_estimators': 50, 'max_depth': 2}\n",
      "0.837 (+/-0.019) for {'n_estimators': 100, 'max_depth': 2}\n",
      "0.846 (+/-0.017) for {'n_estimators': 200, 'max_depth': 2}\n",
      "0.845 (+/-0.016) for {'n_estimators': 500, 'max_depth': 2}\n",
      "0.883 (+/-0.025) for {'n_estimators': 2, 'max_depth': 3}\n",
      "0.872 (+/-0.046) for {'n_estimators': 3, 'max_depth': 3}\n",
      "0.884 (+/-0.020) for {'n_estimators': 5, 'max_depth': 3}\n",
      "0.883 (+/-0.020) for {'n_estimators': 10, 'max_depth': 3}\n",
      "0.887 (+/-0.025) for {'n_estimators': 20, 'max_depth': 3}\n",
      "0.886 (+/-0.027) for {'n_estimators': 30, 'max_depth': 3}\n",
      "0.884 (+/-0.025) for {'n_estimators': 50, 'max_depth': 3}\n",
      "0.886 (+/-0.024) for {'n_estimators': 100, 'max_depth': 3}\n",
      "0.888 (+/-0.027) for {'n_estimators': 200, 'max_depth': 3}\n",
      "0.887 (+/-0.026) for {'n_estimators': 500, 'max_depth': 3}\n",
      "0.889 (+/-0.024) for {'n_estimators': 2, 'max_depth': 5}\n",
      "0.898 (+/-0.016) for {'n_estimators': 3, 'max_depth': 5}\n",
      "0.905 (+/-0.015) for {'n_estimators': 5, 'max_depth': 5}\n",
      "0.904 (+/-0.018) for {'n_estimators': 10, 'max_depth': 5}\n",
      "0.905 (+/-0.019) for {'n_estimators': 20, 'max_depth': 5}\n",
      "0.908 (+/-0.025) for {'n_estimators': 30, 'max_depth': 5}\n",
      "0.906 (+/-0.021) for {'n_estimators': 50, 'max_depth': 5}\n",
      "0.907 (+/-0.020) for {'n_estimators': 100, 'max_depth': 5}\n",
      "0.908 (+/-0.021) for {'n_estimators': 200, 'max_depth': 5}\n",
      "0.909 (+/-0.022) for {'n_estimators': 500, 'max_depth': 5}\n",
      "0.890 (+/-0.018) for {'n_estimators': 2, 'max_depth': 7}\n",
      "0.903 (+/-0.031) for {'n_estimators': 3, 'max_depth': 7}\n",
      "0.906 (+/-0.029) for {'n_estimators': 5, 'max_depth': 7}\n",
      "0.913 (+/-0.030) for {'n_estimators': 10, 'max_depth': 7}\n",
      "0.915 (+/-0.028) for {'n_estimators': 20, 'max_depth': 7}\n",
      "0.915 (+/-0.028) for {'n_estimators': 30, 'max_depth': 7}\n",
      "0.915 (+/-0.026) for {'n_estimators': 50, 'max_depth': 7}\n",
      "0.913 (+/-0.025) for {'n_estimators': 100, 'max_depth': 7}\n",
      "0.915 (+/-0.025) for {'n_estimators': 200, 'max_depth': 7}\n",
      "0.914 (+/-0.023) for {'n_estimators': 500, 'max_depth': 7}\n",
      "0.870 (+/-0.025) for {'n_estimators': 2, 'max_depth': 10}\n",
      "0.901 (+/-0.022) for {'n_estimators': 3, 'max_depth': 10}\n",
      "0.908 (+/-0.021) for {'n_estimators': 5, 'max_depth': 10}\n",
      "0.915 (+/-0.021) for {'n_estimators': 10, 'max_depth': 10}\n",
      "0.913 (+/-0.025) for {'n_estimators': 20, 'max_depth': 10}\n",
      "0.915 (+/-0.026) for {'n_estimators': 30, 'max_depth': 10}\n",
      "0.916 (+/-0.026) for {'n_estimators': 50, 'max_depth': 10}\n",
      "0.918 (+/-0.021) for {'n_estimators': 100, 'max_depth': 10}\n",
      "0.918 (+/-0.024) for {'n_estimators': 200, 'max_depth': 10}\n",
      "0.918 (+/-0.024) for {'n_estimators': 500, 'max_depth': 10}\n",
      "0.848 (+/-0.035) for {'n_estimators': 2, 'max_depth': 15}\n",
      "0.897 (+/-0.022) for {'n_estimators': 3, 'max_depth': 15}\n",
      "0.903 (+/-0.025) for {'n_estimators': 5, 'max_depth': 15}\n",
      "0.909 (+/-0.022) for {'n_estimators': 10, 'max_depth': 15}\n",
      "0.915 (+/-0.022) for {'n_estimators': 20, 'max_depth': 15}\n",
      "0.919 (+/-0.023) for {'n_estimators': 30, 'max_depth': 15}\n",
      "0.917 (+/-0.022) for {'n_estimators': 50, 'max_depth': 15}\n",
      "0.918 (+/-0.022) for {'n_estimators': 100, 'max_depth': 15}\n",
      "0.919 (+/-0.022) for {'n_estimators': 200, 'max_depth': 15}\n",
      "0.919 (+/-0.020) for {'n_estimators': 500, 'max_depth': 15}\n",
      "0.847 (+/-0.023) for {'n_estimators': 2, 'max_depth': 20}\n",
      "0.894 (+/-0.016) for {'n_estimators': 3, 'max_depth': 20}\n",
      "0.902 (+/-0.019) for {'n_estimators': 5, 'max_depth': 20}\n",
      "0.906 (+/-0.023) for {'n_estimators': 10, 'max_depth': 20}\n",
      "0.913 (+/-0.020) for {'n_estimators': 20, 'max_depth': 20}\n",
      "0.914 (+/-0.018) for {'n_estimators': 30, 'max_depth': 20}\n",
      "0.917 (+/-0.019) for {'n_estimators': 50, 'max_depth': 20}\n",
      "0.921 (+/-0.015) for {'n_estimators': 100, 'max_depth': 20}\n",
      "0.918 (+/-0.020) for {'n_estimators': 200, 'max_depth': 20}\n",
      "0.917 (+/-0.020) for {'n_estimators': 500, 'max_depth': 20}\n",
      "0.847 (+/-0.023) for {'n_estimators': 2, 'max_depth': 30}\n",
      "0.894 (+/-0.016) for {'n_estimators': 3, 'max_depth': 30}\n",
      "0.902 (+/-0.019) for {'n_estimators': 5, 'max_depth': 30}\n",
      "0.906 (+/-0.023) for {'n_estimators': 10, 'max_depth': 30}\n",
      "0.913 (+/-0.018) for {'n_estimators': 20, 'max_depth': 30}\n",
      "0.915 (+/-0.017) for {'n_estimators': 30, 'max_depth': 30}\n",
      "0.916 (+/-0.021) for {'n_estimators': 50, 'max_depth': 30}\n",
      "0.920 (+/-0.016) for {'n_estimators': 100, 'max_depth': 30}\n",
      "0.918 (+/-0.018) for {'n_estimators': 200, 'max_depth': 30}\n",
      "0.917 (+/-0.019) for {'n_estimators': 500, 'max_depth': 30}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print(gs.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on training set:\")\n",
    "print()\n",
    "means = gs.cv_results_['mean_test_score']\n",
    "stds = gs.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, gs.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013881</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.775611</td>\n",
       "      <td>0.777671</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{u'n_estimators': 2, u'max_depth': 1}</td>\n",
       "      <td>82</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>0.773695</td>\n",
       "      <td>0.782967</td>\n",
       "      <td>0.776862</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.780295</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.777206</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.780295</td>\n",
       "      <td>0.003265</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.009229</td>\n",
       "      <td>0.002468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021396</td>\n",
       "      <td>0.003766</td>\n",
       "      <td>0.767921</td>\n",
       "      <td>0.770667</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>{u'n_estimators': 3, u'max_depth': 1}</td>\n",
       "      <td>90</td>\n",
       "      <td>0.776406</td>\n",
       "      <td>0.767170</td>\n",
       "      <td>0.782967</td>\n",
       "      <td>0.776862</td>\n",
       "      <td>0.758242</td>\n",
       "      <td>0.768967</td>\n",
       "      <td>0.758242</td>\n",
       "      <td>0.765877</td>\n",
       "      <td>0.763736</td>\n",
       "      <td>0.774459</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>0.004262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.032885</td>\n",
       "      <td>0.005901</td>\n",
       "      <td>0.809942</td>\n",
       "      <td>0.811041</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>{u'n_estimators': 5, u'max_depth': 1}</td>\n",
       "      <td>79</td>\n",
       "      <td>0.823045</td>\n",
       "      <td>0.808379</td>\n",
       "      <td>0.815934</td>\n",
       "      <td>0.810161</td>\n",
       "      <td>0.809066</td>\n",
       "      <td>0.811878</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.810161</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.814624</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.008830</td>\n",
       "      <td>0.002106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.049651</td>\n",
       "      <td>0.008640</td>\n",
       "      <td>0.774238</td>\n",
       "      <td>0.776160</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{u'n_estimators': 10, u'max_depth': 1}</td>\n",
       "      <td>84</td>\n",
       "      <td>0.792867</td>\n",
       "      <td>0.774725</td>\n",
       "      <td>0.773352</td>\n",
       "      <td>0.768623</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.780295</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.776862</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.780295</td>\n",
       "      <td>0.011606</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>0.009536</td>\n",
       "      <td>0.004326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.093227</td>\n",
       "      <td>0.014822</td>\n",
       "      <td>0.775062</td>\n",
       "      <td>0.776092</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>{u'n_estimators': 20, u'max_depth': 1}</td>\n",
       "      <td>83</td>\n",
       "      <td>0.792867</td>\n",
       "      <td>0.774725</td>\n",
       "      <td>0.782967</td>\n",
       "      <td>0.774116</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.779609</td>\n",
       "      <td>0.765110</td>\n",
       "      <td>0.774116</td>\n",
       "      <td>0.765110</td>\n",
       "      <td>0.777892</td>\n",
       "      <td>0.022935</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>0.011062</td>\n",
       "      <td>0.002249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.013881         0.002632         0.775611          0.777671   \n",
       "1       0.021396         0.003766         0.767921          0.770667   \n",
       "2       0.032885         0.005901         0.809942          0.811041   \n",
       "3       0.049651         0.008640         0.774238          0.776160   \n",
       "4       0.093227         0.014822         0.775062          0.776092   \n",
       "\n",
       "  param_max_depth param_n_estimators                                  params  \\\n",
       "0               1                  2   {u'n_estimators': 2, u'max_depth': 1}   \n",
       "1               1                  3   {u'n_estimators': 3, u'max_depth': 1}   \n",
       "2               1                  5   {u'n_estimators': 5, u'max_depth': 1}   \n",
       "3               1                 10  {u'n_estimators': 10, u'max_depth': 1}   \n",
       "4               1                 20  {u'n_estimators': 20, u'max_depth': 1}   \n",
       "\n",
       "   rank_test_score  split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0               82           0.790123            0.773695           0.782967   \n",
       "1               90           0.776406            0.767170           0.782967   \n",
       "2               79           0.823045            0.808379           0.815934   \n",
       "3               84           0.792867            0.774725           0.773352   \n",
       "4               83           0.792867            0.774725           0.782967   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  \\\n",
       "0            0.776862           0.769231            0.780295   \n",
       "1            0.776862           0.758242            0.768967   \n",
       "2            0.810161           0.809066            0.811878   \n",
       "3            0.768623           0.769231            0.780295   \n",
       "4            0.774116           0.769231            0.779609   \n",
       "\n",
       "   split3_test_score  split3_train_score  split4_test_score  \\\n",
       "0           0.767857            0.777206           0.767857   \n",
       "1           0.758242            0.765877           0.763736   \n",
       "2           0.803571            0.810161           0.798077   \n",
       "3           0.767857            0.776862           0.767857   \n",
       "4           0.765110            0.774116           0.765110   \n",
       "\n",
       "   split4_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "0            0.780295      0.003265        0.000590        0.009229   \n",
       "1            0.774459      0.001953        0.000608        0.010032   \n",
       "2            0.814624      0.000425        0.000182        0.008830   \n",
       "3            0.780295      0.011606        0.002127        0.009536   \n",
       "4            0.777892      0.022935        0.003086        0.011062   \n",
       "\n",
       "   std_train_score  \n",
       "0         0.002468  \n",
       "1         0.004262  \n",
       "2         0.002106  \n",
       "3         0.004326  \n",
       "4         0.002249  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(gs.cv_results_)\n",
    "cv_results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.775611</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.767921</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.809942</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.774238</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.775062</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.769294</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.770393</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.769569</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.771217</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.770942</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.885471</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.880253</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.873386</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.828619</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.806647</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_score param_max_depth param_n_estimators\n",
       "0          0.775611               1                  2\n",
       "1          0.767921               1                  3\n",
       "2          0.809942               1                  5\n",
       "3          0.774238               1                 10\n",
       "4          0.775062               1                 20\n",
       "5          0.769294               1                 30\n",
       "6          0.770393               1                 50\n",
       "7          0.769569               1                100\n",
       "8          0.771217               1                200\n",
       "9          0.770942               1                500\n",
       "10         0.885471               2                  2\n",
       "11         0.880253               2                  3\n",
       "12         0.873386               2                  5\n",
       "13         0.828619               2                 10\n",
       "14         0.806647               2                 20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results[['mean_test_score', 'param_max_depth', 'param_n_estimators']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.920626</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.920077</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.919253</td>\n",
       "      <td>15</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.918704</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.918704</td>\n",
       "      <td>15</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_score param_max_depth param_n_estimators\n",
       "77         0.920626              20                100\n",
       "87         0.920077              30                100\n",
       "69         0.919253              15                500\n",
       "65         0.918704              15                 30\n",
       "68         0.918704              15                200"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results[['mean_test_score', 'param_max_depth', 'param_n_estimators']].sort_values(by='mean_test_score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAHqCAYAAAAH7ohlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd785aac090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "this_cv_results = cv_results[cv_results['param_n_estimators'] == 10]\n",
    "plt.errorbar(this_cv_results['param_max_depth'],\n",
    "             this_cv_results['mean_test_score']);\n",
    "plt.xlabel('max depth')\n",
    "plt.ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsoAAAHxCAYAAAB0w5qzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7959a1450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "this_cv_results = cv_results[cv_results['param_max_depth'] == 15]\n",
    "plt.errorbar(this_cv_results['param_n_estimators'],\n",
    "             this_cv_results['mean_test_score']);\n",
    "plt.xlabel('nb estimators')\n",
    "plt.ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's automate the search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All wrapper-type hyperparameter optimization will need a function with a parameter that represents the hyperparameter, and which returns a score (usually to be minimized). The following function thus trains a random forest classifier with two hyperparameters, stored in a pair <code>x_int</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "def objective(params):\n",
    "    global i\n",
    "    i += 1\n",
    "    print(params)\n",
    "    clf = RandomForestClassifier(n_estimators=300,\n",
    "                                 max_depth=int(params['max_depth']),\n",
    "                                 max_features=params['max_features'])\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=3)\n",
    "    score = np.mean(scores)\n",
    "    print(\"SCORE: %s\" % score)\n",
    "    df_result_hyperopt.loc[i, ['score'] + list(params.keys())] = \\\n",
    "        [score] + list(params.values())\n",
    "    return {'loss': 1. - score, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 0.75, 'max_depth': 11.0}\n",
      "SCORE: 0.915384615385\n",
      "{'max_features': 0.75, 'max_depth': 14.0}\n",
      "SCORE: 0.915934065934\n",
      "{'max_features': 0.9500000000000001, 'max_depth': 3.0}\n",
      "SCORE: 0.896703296703\n",
      "{'max_features': 0.8500000000000001, 'max_depth': 19.0}\n",
      "SCORE: 0.915384615385\n",
      "{'max_features': 0.75, 'max_depth': 12.0}\n",
      "SCORE: 0.914835164835\n",
      "{'max_features': 0.9, 'max_depth': 5.0}\n",
      "SCORE: 0.903846153846\n",
      "{'max_features': 0.9500000000000001, 'max_depth': 17.0}\n",
      "SCORE: 0.915934065934\n",
      "{'max_features': 0.7000000000000001, 'max_depth': 15.0}\n",
      "SCORE: 0.915384615385\n",
      "{'max_features': 0.9500000000000001, 'max_depth': 16.0}\n",
      "SCORE: 0.915384615385\n",
      "{'max_features': 0.8500000000000001, 'max_depth': 12.0}\n",
      "SCORE: 0.914835164835\n",
      "{'max_features': 0.75, 'max_depth': 28.0}\n",
      "SCORE: 0.915384615385\n",
      "{'max_features': 0.9500000000000001, 'max_depth': 27.0}\n",
      "SCORE: 0.915384615385\n",
      "{'max_features': 0.75, 'max_depth': 18.0}\n",
      "SCORE: 0.915384615385\n",
      "{'max_features': 0.9500000000000001, 'max_depth': 24.0}\n",
      "SCORE: 0.915384615385\n",
      "{'max_features': 0.8, 'max_depth': 23.0}\n",
      "SCORE: 0.915384615385\n",
      "Best: {'max_features': 0.75, 'max_depth': 14.0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 2, 30, 1),\n",
    "    'max_features': hp.quniform('max_features', 0.7, 1, 0.05)\n",
    "}\n",
    "\n",
    "df_result_hyperopt = pd.DataFrame(\n",
    "    columns=['score'] + list(space.keys()))\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest,\n",
    "            max_evals=15, trials=trials)\n",
    "\n",
    "print(\"Best: %s\" % best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.915934</td>\n",
       "      <td>0.75</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.915934</td>\n",
       "      <td>0.95</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.85</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.7</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.95</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.75</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.95</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.75</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.95</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       score max_features max_depth\n",
       "2   0.915934         0.75        14\n",
       "7   0.915934         0.95        17\n",
       "1   0.915385         0.75        11\n",
       "4   0.915385         0.85        19\n",
       "8   0.915385          0.7        15\n",
       "9   0.915385         0.95        16\n",
       "11  0.915385         0.75        28\n",
       "12  0.915385         0.95        27\n",
       "13  0.915385         0.75        18\n",
       "14  0.915385         0.95        24"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result_hyperopt.sort_values(by='score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
